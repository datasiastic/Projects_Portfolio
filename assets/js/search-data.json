{
  
    
        "post0": {
            "title": "Predicting Stock Price with Machine Learning Algorithm",
            "content": "Importing Dependencies . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns import math import datetime %matplotlib inline . Loading Google Dataset . Visualizing some useful statistic about DataFrame . train_data = pd.read_csv(&quot;GOOGL.csv&quot;) train_data.head() . Date Open High Low Close Adj Close Volume . 0 2009-05-22 | 198.528534 | 199.524521 | 196.196198 | 196.946945 | 196.946945 | 3433700 | . 1 2009-05-26 | 196.171173 | 202.702698 | 195.195190 | 202.382385 | 202.382385 | 6202700 | . 2 2009-05-27 | 203.023026 | 206.136139 | 202.607605 | 202.982986 | 202.982986 | 6062500 | . 3 2009-05-28 | 204.544540 | 206.016022 | 202.507507 | 205.405411 | 205.405411 | 5332200 | . 4 2009-05-29 | 206.261261 | 208.823822 | 205.555557 | 208.823822 | 208.823822 | 5291100 | . train_data.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 2335 entries, 0 to 2334 Data columns (total 7 columns): # Column Non-Null Count Dtype -- -- 0 Date 2335 non-null object 1 Open 2335 non-null float64 2 High 2335 non-null float64 3 Low 2335 non-null float64 4 Close 2335 non-null float64 5 Adj Close 2335 non-null float64 6 Volume 2335 non-null int64 dtypes: float64(5), int64(1), object(1) memory usage: 127.8+ KB . train_data.describe() . Open High Low Close Adj Close Volume . count 2335.000000 | 2335.000000 | 2335.000000 | 2335.000000 | 2335.000000 | 2.335000e+03 | . mean 550.725409 | 555.136744 | 545.908515 | 550.662757 | 550.662757 | 3.764886e+06 | . std 278.136894 | 280.349025 | 275.855161 | 278.228484 | 278.228484 | 2.764696e+06 | . min 196.171173 | 199.524521 | 195.195190 | 196.946945 | 196.946945 | 5.206000e+05 | . 25% 299.286773 | 301.759247 | 297.197204 | 299.144135 | 299.144135 | 1.734650e+06 | . 50% 526.211182 | 532.780029 | 521.909973 | 527.767761 | 527.767761 | 3.250200e+06 | . 75% 757.760010 | 763.745025 | 752.024994 | 758.524993 | 758.524993 | 4.891800e+06 | . max 1289.119995 | 1291.439941 | 1263.000000 | 1285.500000 | 1285.500000 | 2.961990e+07 | . We&#39;re going to use the Adj.Close to predict the stock prices at the end of the day. . #From the train_data.info(), we can see that the train_data.Date has an object data type, so we need to convert it into a datetime object train_data[&quot;Date&quot;]= pd.to_datetime(train_data[&quot;Date&quot;]) train_data.set_index(&quot;Date&quot;, inplace=True ) train_data.head() . Open High Low Close Adj Close Volume . Date . 2009-05-22 198.528534 | 199.524521 | 196.196198 | 196.946945 | 196.946945 | 3433700 | . 2009-05-26 196.171173 | 202.702698 | 195.195190 | 202.382385 | 202.382385 | 6202700 | . 2009-05-27 203.023026 | 206.136139 | 202.607605 | 202.982986 | 202.982986 | 6062500 | . 2009-05-28 204.544540 | 206.016022 | 202.507507 | 205.405411 | 205.405411 | 5332200 | . 2009-05-29 206.261261 | 208.823822 | 205.555557 | 208.823822 | 208.823822 | 5291100 | . train_data[&quot;Year&quot;]= train_data.index.year train_data[&quot;Month&quot;]= train_data.index.month train_data[&quot;Weekday name&quot;]= train_data.index.day_name() . train_data.head() . Open High Low Close Adj Close Volume Year Month Weekday name . Date . 2009-05-22 198.528534 | 199.524521 | 196.196198 | 196.946945 | 196.946945 | 3433700 | 2009 | 5 | Friday | . 2009-05-26 196.171173 | 202.702698 | 195.195190 | 202.382385 | 202.382385 | 6202700 | 2009 | 5 | Tuesday | . 2009-05-27 203.023026 | 206.136139 | 202.607605 | 202.982986 | 202.982986 | 6062500 | 2009 | 5 | Wednesday | . 2009-05-28 204.544540 | 206.016022 | 202.507507 | 205.405411 | 205.405411 | 5332200 | 2009 | 5 | Thursday | . 2009-05-29 206.261261 | 208.823822 | 205.555557 | 208.823822 | 208.823822 | 5291100 | 2009 | 5 | Friday | . Exploratory Data Analysis . plt.figure(figsize=(16,8)) train_data[&quot;Adj Close&quot;].plot() train_data[&quot;Adj Close&quot;].rolling(window=90).mean().plot() plt.xlabel(&quot;Date&quot;) plt.ylabel(&quot;Close Price (USD)&quot;) plt.title(&quot;Close Price History&quot;) . Text(0.5, 1.0, &#39;Close Price History&#39;) . train_data.boxplot(column= [&quot;Adj Close&quot;]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f515b446190&gt; . train_data[[&quot;Open&quot;,&quot;High&quot;,&quot;Low&quot;,&quot;Close&quot;,&quot;Volume&quot;]].plot(figsize=(12,8),subplots=True) . array([&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f515af76190&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f515af2acd0&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f515aeef0d0&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f515af28490&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f515aede850&gt;], dtype=object) . corr=train_data.corr() sns.heatmap(corr, annot= True) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f515afda1d0&gt; . it shows that Adj. Close, Open, High, Close and Low have a very high correlation. So we have to drop these values. | . Preprocessing . Adding new Features to the Dataset . Due to the high correlation, it is more convinient to add some features to the dataset. | HL_PCT calculates for the high-low percentage for each day and the PCT_change calculatesfor the open-close percentage for each day. | . train_data[&quot;HL_PCT&quot;]= (train_data[&quot;High&quot;] - train_data[&quot;Low&quot;])/ train_data[&quot;Low&quot;]*100 #High Low percentage train_data[&quot;PCT_change&quot;]= (train_data[&quot;Close&quot;]- train_data[&quot;Open&quot;])/train_data[&quot;Open&quot;]*100 #Open Close percentage . train_data.isnull().sum() . Open 0 High 0 Low 0 Close 0 Adj Close 0 Volume 0 Year 0 Month 0 Weekday name 0 HL_PCT 0 PCT_change 0 dtype: int64 . df = train_data[[&quot;Adj Close&quot;, &quot;HL_PCT&quot;, &quot;PCT_change&quot;, &quot;Volume&quot;]] . forecast_out = int(math.ceil(len(df)*0.05)) #forcasting out 5% of the entire dataset print(forecast_out) . 117 . df[&quot;Prediction&quot;]= df[&quot;Adj Close&quot;].shift(-forecast_out) df[&quot;Prediction&quot;] . /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy &#34;&#34;&#34;Entry point for launching an IPython kernel. . Date 2009-05-22 275.825836 2009-05-26 281.536530 2009-05-27 283.663666 2009-05-28 285.565552 2009-05-29 284.209198 ... 2018-08-23 NaN 2018-08-24 NaN 2018-08-27 NaN 2018-08-28 NaN 2018-08-29 NaN Name: Prediction, Length: 2335, dtype: float64 . Model Building . from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LinearRegression, Ridge from sklearn.ensemble import RandomForestRegressor from sklearn.svm import SVR from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_log_error . #Now we scale the data between -1 and 1 in order to pu all the column in the same range. #We will be using the StandardSacler fuction from the preprocessing module of sklearn librairy scaler = StandardScaler() . X = np.array(df.drop([&quot;Prediction&quot;],1)) scaler.fit(X) X = scaler.transform(X) . Picking The Data to be Predicted . We have succesfully scaled the data. | Previously we&#39;ve included a new column called &quot;Prediction&quot; to our dataset wich contains forcasted out values. -Also, we&#39;ve made a room for 117 new predictions. -So we are going to pick all rows in the dataset excluding the remaining 117 rows as our training data, and use the remaining 117 rows as the data to be predicted. | . X_Predictions = X[-forecast_out:]#data to be predicted X = X[:-forecast_out]#data to be trained . X.shape . (2218, 4) . df.dropna(inplace=True) . /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy . y = np.array(df[&quot;Prediction&quot;]) y.shape . (2218,) . X_train, X_test, y_train, y_test = train_test_split(X, y , test_size = 0.2, random_state=42) . Training the Models . Applying different linear regression models and see which gives the best accuracy. | . Linear Regression . lr = LinearRegression() lr.fit(X_train,y_train) . LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False) . lr_confidence = lr.score(X_test, y_test) lr_confidence . 0.9558130106873121 . Random Forest . rf = RandomForestRegressor() rf.fit(X_train,y_train) . RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion=&#39;mse&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False) . rf_confidence = rf.score(X_test, y_test) rf_confidence . 0.9740319158929015 . Ridge . rg = Ridge() rg.fit(X_train, y_train) . Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None, normalize=False, random_state=None, solver=&#39;auto&#39;, tol=0.001) . rg_confidence = rg.score(X_test, y_test) rg_confidence . 0.9558232815861608 . Support Vector Regressor (SVR) . svr = SVR() svr.fit(X_train, y_train) . SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=&#39;scale&#39;, kernel=&#39;rbf&#39;, max_iter=-1, shrinking=True, tol=0.001, verbose=False) . svr_confidence = svr.score(X_test, y_test) svr_confidence . 0.6725552349692899 . Visualizing wich model have the best accuracy . names =[&quot;Linear Regression&quot;, &quot;Random Forest&quot;, &quot;Ridge&quot;, &quot;SVR&quot;] columns = [&quot;models&quot;, &quot;accuracy&quot;] scores = [lr_confidence, rf_confidence, rg_confidence, svr_confidence] alg_vs_score = pd.DataFrame([[x,y] for x,y in zip(names,scores)], columns= columns) alg_vs_score . models accuracy . 0 Linear Regression | 0.955813 | . 1 Random Forest | 0.974032 | . 2 Ridge | 0.955823 | . 3 SVR | 0.672555 | . sns.barplot(data = alg_vs_score, x= &quot;models&quot;, y=&quot;accuracy&quot;) plt.title(&#39;Performance of Different Models&#39;) plt.xticks(rotation=&#39;vertical&#39;) . (array([0, 1, 2, 3]), &lt;a list of 4 Text major ticklabel objects&gt;) . Adding the Predicted Data To The Dataset . last_date = df.index[-1] last_unix = last_date.timestamp() one_day = 86400 next_unix = last_unix + one_day . forecast_set = rf.predict(X_Predictions) df[&quot;Forecast&quot;] = np.nan . /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy . for i in forecast_set: next_date = datetime.datetime.fromtimestamp(next_unix) next_unix += 86400 df.loc[next_date] = [np.nan for _ in range(len(df.columns)-1)]+[i] . /usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy iloc._setitem_with_indexer(indexer, value) . plt.figure(figsize=(18, 8)) df[&#39;Adj Close&#39;].plot() df[&#39;Forecast&#39;].plot() plt.legend(loc=4) plt.xlabel(&#39;Date&#39;) plt.ylabel(&#39;Price&#39;) plt.show() . df[&#39;Forecast&#39;].plot() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f514a1844d0&gt; .",
            "url": "https://datasiastic.github.io/Projects_Portfolio/time-serie/linear-regression/svr/ridge/random-forest/googl/scikit-learn/2021/09/28/TS_FC_GOOGL.html",
            "relUrl": "/time-serie/linear-regression/svr/ridge/random-forest/googl/scikit-learn/2021/09/28/TS_FC_GOOGL.html",
            "date": " • Sep 28, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Building a Stock Price Prediction Model with KNN",
            "content": "Importing dependecies and Apple dataset . import pandas as pd from datetime import datetime import pandas_datareader.data as web from pandas import Series, DataFrame start_date = &quot;2010-01-01&quot; today = datetime.today().strftime(&quot;%Y-%m-%d&quot;) df = web.DataReader(&quot;AAPL&quot;, &#39;yahoo&#39;, start = start_date, end = today) df.tail() . High Low Open Close Volume Adj Close . Date . 2020-12-23 132.429993 | 130.779999 | 132.160004 | 130.960007 | 88223700.0 | 130.960007 | . 2020-12-24 133.460007 | 131.100006 | 131.320007 | 131.970001 | 54930100.0 | 131.970001 | . 2020-12-28 137.339996 | 133.509995 | 133.990005 | 136.690002 | 124486200.0 | 136.690002 | . 2020-12-29 138.789993 | 134.339996 | 138.050003 | 134.869995 | 121047300.0 | 134.869995 | . 2020-12-30 135.990005 | 133.399994 | 135.580002 | 133.720001 | 96292700.0 | 133.720001 | . Calculating the Moving Average . df[&quot;M.Avg&quot;]= df[&quot;Adj Close&quot;].rolling(window=100).mean() df.tail() . High Low Open Close Volume Adj Close M.Avg . Date . 2020-12-23 132.429993 | 130.779999 | 132.160004 | 130.960007 | 88223700.0 | 130.960007 | 117.860380 | . 2020-12-24 133.460007 | 131.100006 | 131.320007 | 131.970001 | 54930100.0 | 131.970001 | 118.087289 | . 2020-12-28 137.339996 | 133.509995 | 133.990005 | 136.690002 | 124486200.0 | 136.690002 | 118.357437 | . 2020-12-29 138.789993 | 134.339996 | 138.050003 | 134.869995 | 121047300.0 | 134.869995 | 118.571120 | . 2020-12-30 135.990005 | 133.399994 | 135.580002 | 133.720001 | 96292700.0 | 133.720001 | 118.799109 | . Visualizing the data price &amp; M.avg . import matplotlib.pyplot as plt plt.style.use(&quot;seaborn&quot;) import matplotlib as mpl mpl.rc(&#39;figure&#39;, figsize=(16, 8)) df[[&quot;Adj Close&quot;,&quot;M.Avg&quot;]].plot() plt.show() . Calculating &amp; Visualizing the Daily Returns . returns = df[&quot;Adj Close&quot;].pct_change() returns= pd.DataFrame(returns) returns . Adj Close . Date . 2010-01-04 NaN | . 2010-01-05 0.001729 | . 2010-01-06 -0.015906 | . 2010-01-07 -0.001849 | . 2010-01-08 0.006648 | . ... ... | . 2020-12-23 -0.006976 | . 2020-12-24 0.007712 | . 2020-12-28 0.035766 | . 2020-12-29 -0.013315 | . 2020-12-30 -0.008527 | . 2768 rows × 1 columns . returns.plot() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f0394b8c978&gt; . Bulding the portfolio . dfcomp = web.DataReader([&#39;AAPL&#39;,&#39;AMZN&#39;,&#39;NFLX&#39;,&#39;GOOG&#39;,&#39;MSFT&#39;,&#39;FB&#39;], &#39;yahoo&#39;, start = start_date, end = today)[&quot;Adj Close&quot;] dfcomp . Symbols AAPL AMZN NFLX GOOG MSFT FB . Date . 2010-01-04 6.593426 | 133.899994 | 7.640000 | 312.204773 | 24.105360 | NaN | . 2010-01-05 6.604825 | 134.690002 | 7.358572 | 310.829926 | 24.113148 | NaN | . 2010-01-06 6.499768 | 132.250000 | 7.617143 | 302.994293 | 23.965164 | NaN | . 2010-01-07 6.487752 | 130.000000 | 7.485714 | 295.940735 | 23.715933 | NaN | . 2010-01-08 6.530883 | 133.520004 | 7.614286 | 299.885956 | 23.879499 | NaN | . ... ... | ... | ... | ... | ... | ... | . 2020-12-23 130.960007 | 3185.270020 | 514.479980 | 1732.380005 | 221.020004 | 268.109985 | . 2020-12-24 131.970001 | 3172.689941 | 513.969971 | 1738.849976 | 222.750000 | 267.399994 | . 2020-12-28 136.690002 | 3283.959961 | 519.119995 | 1776.089966 | 224.960007 | 277.000000 | . 2020-12-29 134.869995 | 3322.000000 | 530.869995 | 1758.719971 | 224.149994 | 276.779999 | . 2020-12-30 133.720001 | 3285.850098 | 524.590027 | 1739.520020 | 221.679993 | 271.869995 | . 2768 rows × 6 columns . title = &#39;portfolio Adj. close price history&#39; #Create and plot the graph for c in dfcomp.columns.values: plt.plot(dfcomp[c], label = c) #plt.plot(dfcomp[[&quot;AAPL&quot;,&quot;AMZN&quot;,&quot;NFLX&quot;,&quot;FB&quot;,&quot;MSFT&quot;]]) plt.title(title) plt.xlabel(&quot;Date&quot;, fontsize= 16) plt.ylabel(&quot;Adj.price USD ($)&quot;, fontsize= 16) plt.legend(dfcomp.columns.values, loc=&#39;upper left&#39;) plt.show() . Calculating the portfolio correlation . pfrets = dfcomp.pct_change() pfcorr = pfrets.corr() pfcorr . Symbols AAPL AMZN NFLX GOOG MSFT FB . Symbols . AAPL 1.000000 | 0.448198 | 0.251906 | 0.527316 | 0.556531 | 0.393691 | . AMZN 0.448198 | 1.000000 | 0.412491 | 0.575691 | 0.529254 | 0.451393 | . NFLX 0.251906 | 0.412491 | 1.000000 | 0.330493 | 0.303393 | 0.286692 | . GOOG 0.527316 | 0.575691 | 0.330493 | 1.000000 | 0.611136 | 0.495600 | . MSFT 0.556531 | 0.529254 | 0.303393 | 0.611136 | 1.000000 | 0.415975 | . FB 0.393691 | 0.451393 | 0.286692 | 0.495600 | 0.415975 | 1.000000 | . plt.scatter(pfrets.GOOG, pfrets.AMZN) plt.xlabel(&#39;Returns GOOG&#39;) plt.ylabel(&#39;Returns NFLX&#39;) . Text(0, 0.5, &#39;Returns NFLX&#39;) . from pandas.plotting import scatter_matrix scatter_matrix(pfrets, diagonal=&#39;kde&#39;, figsize=(10, 10)) . array([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f03948f6630&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f0394732780&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f0394b5add8&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f0394a5f470&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f0394a9aac8&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f0394aa2160&gt;], [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f0394ae57b8&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f039478bdd8&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f039478be48&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f03949aab00&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f03947e8198&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f03948197f0&gt;], [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f039470be48&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f03946c74e0&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f0394678b38&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f03946341d0&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f03945e5828&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f039461d278&gt;], [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f03945cc9e8&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f0394589198&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f0394538908&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f03944f60b8&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f03944a2828&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f03944d4f98&gt;], [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f039448d748&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f039443deb8&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f03943fa668&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f03943a7dd8&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f0394362588&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f0394390cf8&gt;], [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f039434b4a8&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f03942f9c18&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f0394b4a550&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f03949b9278&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f039463ccf8&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f0394946160&gt;]], dtype=object) . plt.imshow(pfcorr, cmap=&#39;hot&#39;, interpolation=&#39;none&#39;) plt.colorbar() plt.xticks(range(len(pfcorr)), pfcorr.columns) plt.yticks(range(len(pfcorr)), pfcorr.columns); . plt.scatter(pfrets.mean(), pfrets.std()) plt.xlabel(&#39;Expected returns&#39;) plt.ylabel(&#39;Risk&#39;) for label, x, y in zip(pfrets.columns, pfrets.mean(), pfrets.std()): plt.annotate( label, xy = (x, y), xytext = (20, -20), textcoords = &#39;offset points&#39;, ha = &#39;right&#39;, va = &#39;bottom&#39;, bbox = dict(boxstyle = &#39;round,pad=0.5&#39;, fc = &#39;yellow&#39;, alpha = 0.5), arrowprops = dict(arrowstyle = &#39;-&gt;&#39;, connectionstyle = &#39;arc3,rad=0&#39;)) . Predicting Stocks Price . Feature Engineering . dfreg = df[[&#39;Adj Close&#39;,&#39;Volume&#39;]] dfreg[&#39;HL_PCT&#39;] = (df[&#39;High&#39;] - df[&#39;Low&#39;]) / df[&#39;Close&#39;] * 100.0 dfreg[&#39;PCT_change&#39;] = (df[&#39;Close&#39;] - df[&#39;Open&#39;]) / df[&#39;Open&#39;] * 100.0 dfreg . /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy This is separate from the ipykernel package so we can avoid doing imports until . Adj Close Volume HL_PCT PCT_change . Date . 2010-01-04 6.593426 | 493729600.0 | 0.990606 | 0.271752 | . 2010-01-05 6.604825 | 601904800.0 | 1.091520 | -0.102519 | . 2010-01-06 6.499768 | 552160000.0 | 2.123523 | -1.590633 | . 2010-01-07 6.487752 | 477131200.0 | 1.400893 | -0.552538 | . 2010-01-08 6.530883 | 447610800.0 | 1.386924 | 0.798864 | . ... ... | ... | ... | ... | . 2020-12-23 130.960007 | 88223700.0 | 1.259922 | -0.907988 | . 2020-12-24 131.970001 | 54930100.0 | 1.788286 | 0.494969 | . 2020-12-28 136.690002 | 124486200.0 | 2.801962 | 2.015073 | . 2020-12-29 134.869995 | 121047300.0 | 3.299471 | -2.303519 | . 2020-12-30 133.720001 | 96292700.0 | 1.936892 | -1.371884 | . 2768 rows × 4 columns . Pre-processing &amp; Cross Validation . We will clean up and process the data using the following steps before putting them into the prediction models: . Drop missing value | Separating the label here, we want to predict the AdjClose | Scale the X so that everyone can have the same distribution for linear regression | Finally We want to find Data Series of late X and early X (train) for model generation and evaluation Separate label and identify it as y | Separation of training and testing of model by cross validation train test split | . import numpy as np import math from sklearn import preprocessing from sklearn.model_selection import train_test_split from sklearn import svm #Drop missing value dfreg.fillna(value= -99999, inplace=True) #Create a variable for predicting &#39;n&#39; days out into the future forecast_out = int(math.ceil(len(dfreg)* 0.01)) #Create another column (the target or the dependent variable) shifted &#39;n&#39; units up dfreg[&#39;Prediction&#39;]= df[[&#39;Adj Close&#39;]].shift(-forecast_out) . /usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:4327: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy downcast=downcast, /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy . X = np.array(dfreg.drop([&#39;Prediction&#39;],1)) . X = preprocessing.scale(X) . /usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_data.py:173: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features. warnings.warn(&#34;Numerical issues were encountered &#34; . X = X [:-forecast_out] X . array([[-1.02057093, 0.90509935, -0.78641583, 0.18451687], [-1.02011453, 1.37688182, -0.70432877, -0.0910894 ], [-1.02432078, 1.15993076, 0.13513761, -1.18691057], ..., [ 3.53198167, -0.85051957, 0.32811589, 0.83893735], [ 3.4955472 , -0.92427756, -0.42714281, -0.11415263], [ 3.44109587, -0.91533213, -0.33790859, -0.37568687]]) . x_forecast = X[-forecast_out:] x_forecast . array([[ 3.68755825, -0.20049764, 2.26571997, 2.64632548], [ 3.55566121, -0.10409719, 2.26337423, -2.46687302], [ 3.55925835, -0.5893691 , 0.69660037, 0.10003536], [ 3.5400734 , -0.75729195, 0.46310651, 1.21873625], [ 3.47252593, -0.74492946, 0.28043004, -1.38781467], [ 3.35102107, -0.72205236, 1.74624838, -2.45874384], [ 3.41217335, -0.70554753, 0.72675126, 0.81458001], [ 3.38659352, -0.85591447, -0.01921017, 0.11063985], [ 3.34182814, -0.80339592, 0.83228552, -1.08145273], [ 3.31345057, -0.88807194, 0.01288473, -0.86972085], [ 3.31385012, -0.76038192, 1.00258356, 0.65613284], [ 3.3758015 , -0.8457492 , 0.31929076, 0.6921576 ], [ 3.15997008, -0.62044105, 1.57521214, -2.47980977], [ 3.32464154, -0.61088374, 1.74420026, 1.91759423], [ 3.06644355, -0.41836196, 1.59845966, -1.47430301], [ 3.0628461 , -0.71233717, 0.9205612 , -0.24506488], [ 3.12959417, -0.778814 , 0.44063649, 0.50818388], [ 3.30985313, -0.64531038, 0.70054945, 0.50697943], [ 3.47292578, -0.69698459, 0.28710278, 0.65866771], [ 3.46752091, -0.74901118, 0.51179591, 0.21468041], [ 3.37263139, -0.5743096 , 2.56167302, -2.57001926], [ 3.35861825, -0.64623541, 0.83469646, 0.252062 ], [ 3.49955091, -0.7584442 , 0.5793967 , 1.42964339], [ 3.48834039, -0.79827447, -0.25479513, -0.26799534], [ 3.4903424 , -0.89239266, -0.36448924, -0.12657191], [ 3.53198167, -0.85051957, 0.32811589, 0.83893735], [ 3.4955472 , -0.92427756, -0.42714281, -0.11415263], [ 3.44109587, -0.91533213, -0.33790859, -0.37568687]]) . y = np.array(dfreg[&#39;Prediction&#39;])[:-forecast_out] y . array([ 6.17349958, 6.26654387, 6.24035645, ..., 136.69000244, 134.86999512, 133.72000122]) . X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2) . K Nearest Neighbor (KNN) . This KNN uses feature similarity to predict values of data points. This ensures that the new point assigned is similar to the points in the data set. To find out similarity, we will extract the points to release the minimum distance (e.g: Euclidean Distance). . from sklearn.neighbors import KNeighborsRegressor . clfknn = KNeighborsRegressor(n_neighbors=2) clfknn.fit(X_train, y_train) . KNeighborsRegressor(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;minkowski&#39;, metric_params=None, n_jobs=None, n_neighbors=2, p=2, weights=&#39;uniform&#39;) . Evaluation . A simple quick and dirty way to evaluate is to use the score method in each trained model. The score method finds the mean accuracy of self.predict(X) with y of the test data set. . confidenceknn = clfknn.score(X_test, y_test) print(&quot;The KNN regression confidence is: &quot;, confidenceknn) . The KNN regression confidence is: 0.9531964601858879 . Predicting the Stock Prices . forecast_set = clfknn.predict(x_forecast) #Create an empty column for the forecast result print(forecast_set) . [117.61442947 114.1742363 121.0399971 123.52500153 115.4559021 119.61592484 125.06499863 120.77249908 118.98090363 115.73831177 125.06499863 119.65032196 119.61592484 118.26601028 117.05093765 120.95000076 119.65032196 121.71532059 119.65032196 114.7003212 112.96133041 121.89031982 123.52500153 132.34000397 131.46500397 119.45999908 131.46500397 132.34000397] . Plotting the Prediction . predictions = forecast_set valid = df[X.shape[0]:] valid[&quot;Prediction&quot;] = predictions plt.figure(figsize=(16,8)) plt.title(&quot;Apple&quot;) plt.xlabel(&quot;Days&quot;) plt.ylabel(&quot;Adj. Close USD ($)&quot;) plt.plot(df[&quot;Adj Close&quot;]) plt.plot(valid[[&quot;Adj Close&quot;,&quot;Prediction&quot;]]) plt.legend([&quot;Orig&quot;,&quot;Valid&quot;,&quot;Pred&quot;]) plt.show() . /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy &#34;&#34;&#34; .",
            "url": "https://datasiastic.github.io/Projects_Portfolio/knn/aapl/scikit-learn/2021/09/13/SPP_with_KNN.html",
            "relUrl": "/knn/aapl/scikit-learn/2021/09/13/SPP_with_KNN.html",
            "date": " • Sep 13, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://datasiastic.github.io/Projects_Portfolio/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://datasiastic.github.io/Projects_Portfolio/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". I&#39;m a Finance Graduate Interested In Machine Learning &amp; its applications in Finance.On this blog page I publish some applications &amp; projects I&#39;ve worked on,and useful ressources to get started in Data Science &amp; Machine Learning .",
          "url": "https://datasiastic.github.io/Projects_Portfolio/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://datasiastic.github.io/Projects_Portfolio/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}