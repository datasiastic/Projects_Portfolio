{
  
    
        "post0": {
            "title": "Predicting Microsoft Stock Price with Keras",
            "content": "Importing dependecies . from IPython.core.debugger import set_trace import pandas as pd import numpy as np import os import matplotlib.pyplot as plt import datetime plt.style.use(style=&quot;seaborn&quot;) %matplotlib inline . Importing the Stock Price Dataset . df = pd.read_csv(&quot;MSFT-1Y-Hourly.csv&quot;, sep=&quot;,&quot;) . Data Exploration . df.isna().any() . date False open False high False low False close False volume False average False barCount False dtype: bool . df.set_index(&quot;date&quot;, drop=True, inplace=True) . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Index: 1753 entries, 2019-08-07 14:30:00 to 2020-08-05 15:00:00 Data columns (total 7 columns): # Column Non-Null Count Dtype -- -- 0 open 1753 non-null float64 1 high 1753 non-null float64 2 low 1753 non-null float64 3 close 1753 non-null float64 4 volume 1753 non-null int64 5 average 1753 non-null float64 6 barCount 1753 non-null int64 dtypes: float64(5), int64(2) memory usage: 109.6+ KB . df . open high low close volume average barCount . date . 2019-08-07 14:30:00 133.80 | 133.83 | 131.82 | 132.89 | 35647 | 132.701 | 17523 | . 2019-08-07 15:00:00 132.87 | 135.20 | 132.64 | 134.75 | 48757 | 134.043 | 26974 | . 2019-08-07 16:00:00 134.74 | 134.92 | 133.52 | 133.88 | 28977 | 134.147 | 17853 | . 2019-08-07 17:00:00 133.89 | 134.06 | 133.07 | 133.90 | 21670 | 133.618 | 13497 | . 2019-08-07 18:00:00 133.89 | 135.24 | 133.83 | 134.83 | 22648 | 134.653 | 12602 | . ... ... | ... | ... | ... | ... | ... | ... | . 2020-08-04 18:00:00 212.13 | 213.15 | 211.76 | 212.99 | 27210 | 212.572 | 13294 | . 2020-08-04 19:00:00 213.01 | 213.21 | 211.13 | 211.97 | 32968 | 212.084 | 18230 | . 2020-08-04 20:00:00 211.97 | 213.37 | 211.25 | 213.35 | 51797 | 212.261 | 28991 | . 2020-08-05 14:30:00 214.89 | 215.00 | 212.07 | 214.19 | 44975 | 213.543 | 20045 | . 2020-08-05 15:00:00 214.21 | 214.40 | 213.22 | 213.64 | 10402 | 213.800 | 5254 | . 1753 rows × 7 columns . df = df[[&quot;close&quot;]] . df.describe() . close . count 1753.000000 | . mean 164.330610 | . std 23.125225 | . min 132.670000 | . 25% 143.320000 | . 50% 159.750000 | . 75% 183.390000 | . max 216.540000 | . Visualizing Microsoft close price . plt.figure(1, figsize=(16, 8)) plt.plot(df.close) . [&lt;matplotlib.lines.Line2D at 0x1c76637e460&gt;] . Adding some Features . df[&quot;returns&quot;] = df.close.pct_change() . &lt;ipython-input-11-6d4567e8d880&gt;:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy df[&#34;returns&#34;] = df.close.pct_change() . df.head() . close returns . date . 2019-08-07 14:30:00 132.89 | NaN | . 2019-08-07 15:00:00 134.75 | 0.013997 | . 2019-08-07 16:00:00 133.88 | -0.006456 | . 2019-08-07 17:00:00 133.90 | 0.000149 | . 2019-08-07 18:00:00 134.83 | 0.006945 | . Setting the Log returns as a Feature . df[&quot;log_returns&quot;] = np.log(1 + df[&quot;returns&quot;]) . &lt;ipython-input-13-fe3b4b6f324b&gt;:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy df[&#34;log_returns&#34;] = np.log(1 + df[&#34;returns&#34;]) . df.head() . close returns log_returns . date . 2019-08-07 14:30:00 132.89 | NaN | NaN | . 2019-08-07 15:00:00 134.75 | 0.013997 | 0.013899 | . 2019-08-07 16:00:00 133.88 | -0.006456 | -0.006477 | . 2019-08-07 17:00:00 133.90 | 0.000149 | 0.000149 | . 2019-08-07 18:00:00 134.83 | 0.006945 | 0.006921 | . Plotting the Log returns . plt.figure(figsize=(16, 8)) plt.plot(df.log_returns) . [&lt;matplotlib.lines.Line2D at 0x1c76806f280&gt;] . df.dropna(inplace=True) X = df[[&quot;close&quot;, &quot;log_returns&quot;]].values . &lt;ipython-input-16-48661d55c8de&gt;:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy df.dropna(inplace=True) . X[:10] . array([[ 1.34750000e+02, 1.38994914e-02], [ 1.33880000e+02, -6.47733345e-03], [ 1.33900000e+02, 1.49376354e-04], [ 1.34830000e+02, 6.92147295e-03], [ 1.35480000e+02, 4.80930230e-03], [ 1.35280000e+02, -1.47732336e-03], [ 1.36850000e+02, 1.15387309e-02], [ 1.37810000e+02, 6.99048940e-03], [ 1.38250000e+02, 3.18771552e-03], [ 1.38330000e+02, 5.78494484e-04]]) . Model Building . from sklearn.preprocessing import MinMaxScaler . Scaling the Features . scaler = MinMaxScaler(feature_range=(0, 1)).fit(X) X_scaled = scaler.transform(X) . X_scaled[:10] . array([[0.02480029, 0.6063657 ], [0.01442709, 0.50534309], [0.01466555, 0.53819647], [0.02575414, 0.57177063], [0.03350423, 0.56129908], [0.03111959, 0.53013175], [0.04983904, 0.59466171], [0.06128532, 0.5721128 ], [0.06653154, 0.55325971], [0.06748539, 0.54032392]]) . y = [x[0] for x in X_scaled] y[:10] . [0.024800286157148133, 0.014427089543340932, 0.014665553833313805, 0.025754143317038736, 0.033504232741147, 0.03111958984142138, 0.049839036604268694, 0.06128532252295238, 0.06653153690234914, 0.06748539406223952] . Splitting the data into trainning &amp; test data . split = int(len(X_scaled) * 0.8) print(split) . 1401 . X_train = X_scaled[:split] X_test = X_scaled[split : len(X_scaled)] y_train = y[:split] y_test = y[split : len(y)] . assert len(X_train) == len(y_train) assert len(X_test) == len(y_test) . Labeling: . we want to predict the stock price at a future time. We&#39;ll predict the stock price at time t+1 relative to stock price at time t . As we&#39;re going to use an LSTM architecture, we know that it has memory and that it is maintined by setting a time step basiclly how many steps in the past we want the LSTM to use. . The time step refers to how many steps in the time we want the backpropagation algorithm to use when calculating the gradients for weights updates during training. . So we can use a method to create both the timestep and the ouput variable, t+1 . n = 3 Xtrain = [] Xtest = [] ytrain = [] ytest = [] for i in range(n, len(X_train)): Xtrain.append(X_train[i - n : i, : X_train.shape[1]]) ytrain.append(y_train[i]) for i in range(n, len(X_test)): Xtest.append(X_test[i - n : i, : X_test.shape[1]]) ytest.append(y_test[i]) . df.head() . close returns log_returns . date . 2019-08-07 15:00:00 134.75 | 0.013997 | 0.013899 | . 2019-08-07 16:00:00 133.88 | -0.006456 | -0.006477 | . 2019-08-07 17:00:00 133.90 | 0.000149 | 0.000149 | . 2019-08-07 18:00:00 134.83 | 0.006945 | 0.006921 | . 2019-08-07 19:00:00 135.48 | 0.004821 | 0.004809 | . Xtrain[0] . array([[0.02480029, 0.6063657 ], [0.01442709, 0.50534309], [0.01466555, 0.53819647]]) . ytrain[0] . 0.025754143317038736 . val = np.array(ytrain[0]) val = np.c_[val, np.zeros(val.shape)] . scaler.inverse_transform(val) . array([[ 1.34830000e+02, -1.08407857e-01]]) . In an LSTM network the input for each LSTM layer needs to contain the following network: . The number of observations . The time steps . The features . Therefore we need to add a temporel dimension compared to a classical network (number of observation, number of steps, number of feature per step) . Xtrain, ytrain = (np.array(Xtrain), np.array(ytrain)) Xtrain = np.reshape(Xtrain, (Xtrain.shape[0],Xtrain.shape[1],Xtrain.shape[2])) Xtest, ytest = (np.array(Xtest), np.array(ytest)) Xtest = np.reshape(Xtest, (Xtest.shape[0],Xtest.shape[1],Xtest.shape[2])) . print(Xtrain.shape) print(ytrain.shape) print(&#39;-&#39;) print(Xtest.shape) print(ytest.shape) . (1398, 3, 2) (1398,) - (348, 3, 2) (348,) . LSTM Model . from keras.models import Sequential from keras.layers import LSTM, Dense . model =Sequential() model.add(LSTM(4, input_shape=(Xtrain.shape[1],Xtrain.shape[2]))) model.add(Dense(1)) model.compile(loss=&quot;mean_squared_error&quot;, optimizer=&quot;adam&quot;) model.fit(Xtrain, ytrain, epochs=100, validation_data=(Xtest,ytest), batch_size=16, verbose=1) . Epoch 1/100 88/88 [==============================] - 10s 31ms/step - loss: 0.1529 - val_loss: 0.5727 Epoch 2/100 88/88 [==============================] - 1s 11ms/step - loss: 0.0524 - val_loss: 0.2905 Epoch 3/100 88/88 [==============================] - 1s 10ms/step - loss: 0.0328 - val_loss: 0.2107 Epoch 4/100 88/88 [==============================] - 1s 10ms/step - loss: 0.0282 - val_loss: 0.1800 Epoch 5/100 88/88 [==============================] - 1s 12ms/step - loss: 0.0237 - val_loss: 0.1393 Epoch 6/100 88/88 [==============================] - 1s 10ms/step - loss: 0.0190 - val_loss: 0.1030 Epoch 7/100 88/88 [==============================] - 1s 11ms/step - loss: 0.0137 - val_loss: 0.0664 Epoch 8/100 88/88 [==============================] - 1s 9ms/step - loss: 0.0084 - val_loss: 0.0296 Epoch 9/100 88/88 [==============================] - 1s 9ms/step - loss: 0.0038 - val_loss: 0.0085 Epoch 10/100 88/88 [==============================] - 1s 8ms/step - loss: 0.0012 - val_loss: 0.0011 Epoch 11/100 88/88 [==============================] - 1s 7ms/step - loss: 5.1356e-04 - val_loss: 4.4004e-04 Epoch 12/100 88/88 [==============================] - 1s 7ms/step - loss: 4.4835e-04 - val_loss: 4.5397e-04 Epoch 13/100 88/88 [==============================] - 1s 8ms/step - loss: 4.4176e-04 - val_loss: 4.4671e-04 Epoch 14/100 88/88 [==============================] - 1s 7ms/step - loss: 4.3327e-04 - val_loss: 4.5926e-04 Epoch 15/100 88/88 [==============================] - 1s 6ms/step - loss: 4.2333e-04 - val_loss: 4.7426e-04 Epoch 16/100 88/88 [==============================] - 1s 7ms/step - loss: 4.1812e-04 - val_loss: 5.4229e-04 Epoch 17/100 88/88 [==============================] - 1s 6ms/step - loss: 4.1389e-04 - val_loss: 5.3970e-04 Epoch 18/100 88/88 [==============================] - 1s 6ms/step - loss: 4.0356e-04 - val_loss: 5.6159e-04 Epoch 19/100 88/88 [==============================] - 1s 7ms/step - loss: 3.9753e-04 - val_loss: 6.4844e-04 Epoch 20/100 88/88 [==============================] - 1s 7ms/step - loss: 3.9038e-04 - val_loss: 7.9734e-04 Epoch 21/100 88/88 [==============================] - 1s 6ms/step - loss: 3.8707e-04 - val_loss: 7.7441e-04 Epoch 22/100 88/88 [==============================] - 1s 7ms/step - loss: 3.7957e-04 - val_loss: 8.1438e-04 Epoch 23/100 88/88 [==============================] - 1s 6ms/step - loss: 3.7526e-04 - val_loss: 0.0011 Epoch 24/100 88/88 [==============================] - 1s 7ms/step - loss: 3.6494e-04 - val_loss: 9.8834e-04 Epoch 25/100 88/88 [==============================] - 1s 6ms/step - loss: 3.6789e-04 - val_loss: 0.0012 Epoch 26/100 88/88 [==============================] - 1s 7ms/step - loss: 3.5950e-04 - val_loss: 0.0012 Epoch 27/100 88/88 [==============================] - 1s 6ms/step - loss: 3.5627e-04 - val_loss: 0.0013 Epoch 28/100 88/88 [==============================] - 1s 7ms/step - loss: 3.4922e-04 - val_loss: 0.0014 Epoch 29/100 88/88 [==============================] - 1s 7ms/step - loss: 3.4514e-04 - val_loss: 0.0014 Epoch 30/100 88/88 [==============================] - 1s 7ms/step - loss: 3.4433e-04 - val_loss: 0.0017 Epoch 31/100 88/88 [==============================] - 1s 6ms/step - loss: 3.3925e-04 - val_loss: 0.0015 Epoch 32/100 88/88 [==============================] - 1s 7ms/step - loss: 3.4124e-04 - val_loss: 0.0017 Epoch 33/100 88/88 [==============================] - 1s 6ms/step - loss: 3.3934e-04 - val_loss: 0.0022 Epoch 34/100 88/88 [==============================] - 1s 7ms/step - loss: 3.3443e-04 - val_loss: 0.0022 Epoch 35/100 88/88 [==============================] - 1s 6ms/step - loss: 3.3363e-04 - val_loss: 0.0021 Epoch 36/100 88/88 [==============================] - 1s 7ms/step - loss: 3.3237e-04 - val_loss: 0.0022 Epoch 37/100 88/88 [==============================] - 1s 7ms/step - loss: 3.3179e-04 - val_loss: 0.0028 Epoch 38/100 88/88 [==============================] - 1s 6ms/step - loss: 3.3473e-04 - val_loss: 0.0022 Epoch 39/100 88/88 [==============================] - 1s 6ms/step - loss: 3.2801e-04 - val_loss: 0.0026 Epoch 40/100 88/88 [==============================] - 1s 6ms/step - loss: 3.3199e-04 - val_loss: 0.0026 Epoch 41/100 88/88 [==============================] - 1s 6ms/step - loss: 3.2491e-04 - val_loss: 0.0026 Epoch 42/100 88/88 [==============================] - 1s 6ms/step - loss: 3.2698e-04 - val_loss: 0.0028 Epoch 43/100 88/88 [==============================] - 1s 7ms/step - loss: 3.2235e-04 - val_loss: 0.0026 Epoch 44/100 88/88 [==============================] - 1s 6ms/step - loss: 3.1769e-04 - val_loss: 0.0024 Epoch 45/100 88/88 [==============================] - 1s 7ms/step - loss: 3.2646e-04 - val_loss: 0.0024 Epoch 46/100 88/88 [==============================] - 1s 6ms/step - loss: 3.2070e-04 - val_loss: 0.0026 Epoch 47/100 88/88 [==============================] - 1s 9ms/step - loss: 3.1651e-04 - val_loss: 0.0030 Epoch 48/100 88/88 [==============================] - 1s 7ms/step - loss: 3.1200e-04 - val_loss: 0.0025 Epoch 49/100 88/88 [==============================] - 1s 7ms/step - loss: 3.1635e-04 - val_loss: 0.0027 Epoch 50/100 88/88 [==============================] - 1s 7ms/step - loss: 3.0964e-04 - val_loss: 0.0032 Epoch 51/100 88/88 [==============================] - 1s 7ms/step - loss: 3.1014e-04 - val_loss: 0.0034 Epoch 52/100 88/88 [==============================] - 1s 7ms/step - loss: 3.0779e-04 - val_loss: 0.0030 Epoch 53/100 88/88 [==============================] - 1s 7ms/step - loss: 3.0799e-04 - val_loss: 0.0029 Epoch 54/100 88/88 [==============================] - 1s 7ms/step - loss: 3.0336e-04 - val_loss: 0.0026 Epoch 55/100 88/88 [==============================] - 1s 7ms/step - loss: 3.1482e-04 - val_loss: 0.0023 Epoch 56/100 88/88 [==============================] - 1s 7ms/step - loss: 3.0991e-04 - val_loss: 0.0028 Epoch 57/100 88/88 [==============================] - 1s 6ms/step - loss: 3.0377e-04 - val_loss: 0.0026 Epoch 58/100 88/88 [==============================] - 1s 7ms/step - loss: 3.0418e-04 - val_loss: 0.0029 Epoch 59/100 88/88 [==============================] - 1s 8ms/step - loss: 3.0124e-04 - val_loss: 0.0029 Epoch 60/100 88/88 [==============================] - 1s 7ms/step - loss: 2.9761e-04 - val_loss: 0.0025 Epoch 61/100 88/88 [==============================] - 1s 10ms/step - loss: 2.9867e-04 - val_loss: 0.0027 Epoch 62/100 88/88 [==============================] - 1s 7ms/step - loss: 3.0046e-04 - val_loss: 0.0029 Epoch 63/100 88/88 [==============================] - 1s 8ms/step - loss: 2.9729e-04 - val_loss: 0.0030 Epoch 64/100 88/88 [==============================] - 1s 7ms/step - loss: 3.0713e-04 - val_loss: 0.0027 Epoch 65/100 88/88 [==============================] - 1s 7ms/step - loss: 2.9885e-04 - val_loss: 0.0029 Epoch 66/100 88/88 [==============================] - 1s 6ms/step - loss: 2.9568e-04 - val_loss: 0.0025 Epoch 67/100 88/88 [==============================] - 1s 7ms/step - loss: 3.0262e-04 - val_loss: 0.0033 Epoch 68/100 88/88 [==============================] - 1s 8ms/step - loss: 2.9262e-04 - val_loss: 0.0024 Epoch 69/100 88/88 [==============================] - 1s 8ms/step - loss: 2.9898e-04 - val_loss: 0.0020 Epoch 70/100 88/88 [==============================] - 1s 7ms/step - loss: 2.9041e-04 - val_loss: 0.0034 Epoch 71/100 88/88 [==============================] - 1s 7ms/step - loss: 2.9213e-04 - val_loss: 0.0025 Epoch 72/100 88/88 [==============================] - 1s 7ms/step - loss: 2.9524e-04 - val_loss: 0.0024 Epoch 73/100 88/88 [==============================] - 1s 7ms/step - loss: 2.9529e-04 - val_loss: 0.0026 Epoch 74/100 88/88 [==============================] - 1s 7ms/step - loss: 2.8955e-04 - val_loss: 0.0028 Epoch 75/100 88/88 [==============================] - 1s 7ms/step - loss: 2.9428e-04 - val_loss: 0.0041 Epoch 76/100 88/88 [==============================] - 1s 7ms/step - loss: 3.0405e-04 - val_loss: 0.0028 Epoch 77/100 88/88 [==============================] - 1s 7ms/step - loss: 2.8589e-04 - val_loss: 0.0024 Epoch 78/100 88/88 [==============================] - 1s 6ms/step - loss: 2.8484e-04 - val_loss: 0.0024 Epoch 79/100 88/88 [==============================] - 1s 6ms/step - loss: 2.8276e-04 - val_loss: 0.0027 Epoch 80/100 88/88 [==============================] - 1s 7ms/step - loss: 2.8379e-04 - val_loss: 0.0025 Epoch 81/100 88/88 [==============================] - 1s 7ms/step - loss: 2.8411e-04 - val_loss: 0.0031 Epoch 82/100 88/88 [==============================] - 1s 7ms/step - loss: 2.8286e-04 - val_loss: 0.0032 Epoch 83/100 88/88 [==============================] - 1s 8ms/step - loss: 2.8796e-04 - val_loss: 0.0024 Epoch 84/100 88/88 [==============================] - 1s 6ms/step - loss: 2.8584e-04 - val_loss: 0.0034 Epoch 85/100 88/88 [==============================] - 1s 8ms/step - loss: 2.8866e-04 - val_loss: 0.0025 Epoch 86/100 88/88 [==============================] - 1s 7ms/step - loss: 2.8129e-04 - val_loss: 0.0027 Epoch 87/100 88/88 [==============================] - 1s 7ms/step - loss: 2.8106e-04 - val_loss: 0.0023 Epoch 88/100 88/88 [==============================] - 1s 7ms/step - loss: 2.7909e-04 - val_loss: 0.0025 Epoch 89/100 88/88 [==============================] - 1s 6ms/step - loss: 2.8744e-04 - val_loss: 0.0024 Epoch 90/100 88/88 [==============================] - 1s 7ms/step - loss: 2.7966e-04 - val_loss: 0.0028 Epoch 91/100 88/88 [==============================] - 1s 7ms/step - loss: 2.8353e-04 - val_loss: 0.0025 Epoch 92/100 88/88 [==============================] - 1s 7ms/step - loss: 2.8781e-04 - val_loss: 0.0020 Epoch 93/100 88/88 [==============================] - 1s 7ms/step - loss: 2.8071e-04 - val_loss: 0.0023 Epoch 94/100 88/88 [==============================] - 1s 7ms/step - loss: 2.7348e-04 - val_loss: 0.0020 Epoch 95/100 88/88 [==============================] - 1s 7ms/step - loss: 2.8587e-04 - val_loss: 0.0024 Epoch 96/100 88/88 [==============================] - 1s 7ms/step - loss: 2.7434e-04 - val_loss: 0.0026 Epoch 97/100 88/88 [==============================] - 1s 7ms/step - loss: 2.8425e-04 - val_loss: 0.0020 Epoch 98/100 88/88 [==============================] - 1s 7ms/step - loss: 2.8123e-04 - val_loss: 0.0024 Epoch 99/100 88/88 [==============================] - 1s 7ms/step - loss: 2.8880e-04 - val_loss: 0.0026 Epoch 100/100 88/88 [==============================] - 1s 7ms/step - loss: 2.8185e-04 - val_loss: 0.0021 . &lt;keras.callbacks.History at 0x1c774090820&gt; . model.summary() . Model: &#34;sequential&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= lstm (LSTM) (None, 4) 112 _________________________________________________________________ dense (Dense) (None, 1) 5 ================================================================= Total params: 117 Trainable params: 117 Non-trainable params: 0 _________________________________________________________________ . Model Prediction . trainPredict = model.predict(Xtrain) testPredict = model.predict(Xtest) . trainPredict = np.c_[trainPredict, np.zeros(trainPredict.shape)] testPredict = np.c_[testPredict, np.zeros(testPredict.shape)] . trainPredict = scaler.inverse_transform(trainPredict) trainPredict = [x[0] for x in trainPredict] testPredict = scaler.inverse_transform(testPredict) testPredict = [x[0] for x in testPredict] . print(trainPredict[:5]) print(testPredict[:5]) #Plot the train &amp; test Predict Features . [134.6962815105915, 134.8327444097027, 135.48487375501543, 135.70029776528477, 136.69200200550256] [183.2663044089079, 183.02248115599156, 182.80488287508487, 182.00673590004445, 179.3152119332552] . from sklearn.metrics import mean_squared_error . trainScore = mean_squared_error([x[0][0] for x in Xtrain],trainPredict, squared= False ) print(&quot;Train Score : %.2f RMSE&quot; % (trainScore)) testScore = mean_squared_error([x[0][0] for x in Xtest],testPredict, squared= False ) print(&quot;Train Score : %.2f RMSE&quot; % (testScore)) . Train Score : 156.33 RMSE Train Score : 195.62 RMSE .",
            "url": "https://datasiastic.github.io/Projects_Portfolio/keras/lstm/microsoft/neuralnetwork/2021/09/30/MSFT_LSTM.html",
            "relUrl": "/keras/lstm/microsoft/neuralnetwork/2021/09/30/MSFT_LSTM.html",
            "date": " • Sep 30, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Predicting Stock Price with Machine Learning Algorithm",
            "content": "Importing Dependencies . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns import math import datetime %matplotlib inline . Loading Google Dataset . Visualizing some useful statistic about DataFrame . train_data = pd.read_csv(&quot;GOOGL.csv&quot;) train_data.head() . Date Open High Low Close Adj Close Volume . 0 2009-05-22 | 198.528534 | 199.524521 | 196.196198 | 196.946945 | 196.946945 | 3433700 | . 1 2009-05-26 | 196.171173 | 202.702698 | 195.195190 | 202.382385 | 202.382385 | 6202700 | . 2 2009-05-27 | 203.023026 | 206.136139 | 202.607605 | 202.982986 | 202.982986 | 6062500 | . 3 2009-05-28 | 204.544540 | 206.016022 | 202.507507 | 205.405411 | 205.405411 | 5332200 | . 4 2009-05-29 | 206.261261 | 208.823822 | 205.555557 | 208.823822 | 208.823822 | 5291100 | . train_data.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 2335 entries, 0 to 2334 Data columns (total 7 columns): # Column Non-Null Count Dtype -- -- 0 Date 2335 non-null object 1 Open 2335 non-null float64 2 High 2335 non-null float64 3 Low 2335 non-null float64 4 Close 2335 non-null float64 5 Adj Close 2335 non-null float64 6 Volume 2335 non-null int64 dtypes: float64(5), int64(1), object(1) memory usage: 127.8+ KB . train_data.describe() . Open High Low Close Adj Close Volume . count 2335.000000 | 2335.000000 | 2335.000000 | 2335.000000 | 2335.000000 | 2.335000e+03 | . mean 550.725409 | 555.136744 | 545.908515 | 550.662757 | 550.662757 | 3.764886e+06 | . std 278.136894 | 280.349025 | 275.855161 | 278.228484 | 278.228484 | 2.764696e+06 | . min 196.171173 | 199.524521 | 195.195190 | 196.946945 | 196.946945 | 5.206000e+05 | . 25% 299.286773 | 301.759247 | 297.197204 | 299.144135 | 299.144135 | 1.734650e+06 | . 50% 526.211182 | 532.780029 | 521.909973 | 527.767761 | 527.767761 | 3.250200e+06 | . 75% 757.760010 | 763.745025 | 752.024994 | 758.524993 | 758.524993 | 4.891800e+06 | . max 1289.119995 | 1291.439941 | 1263.000000 | 1285.500000 | 1285.500000 | 2.961990e+07 | . We&#39;re going to use the Adj.Close to predict the stock prices at the end of the day. . #From the train_data.info(), we can see that the train_data.Date has an object data type, so we need to convert it into a datetime object train_data[&quot;Date&quot;]= pd.to_datetime(train_data[&quot;Date&quot;]) train_data.set_index(&quot;Date&quot;, inplace=True ) train_data.head() . Open High Low Close Adj Close Volume . Date . 2009-05-22 198.528534 | 199.524521 | 196.196198 | 196.946945 | 196.946945 | 3433700 | . 2009-05-26 196.171173 | 202.702698 | 195.195190 | 202.382385 | 202.382385 | 6202700 | . 2009-05-27 203.023026 | 206.136139 | 202.607605 | 202.982986 | 202.982986 | 6062500 | . 2009-05-28 204.544540 | 206.016022 | 202.507507 | 205.405411 | 205.405411 | 5332200 | . 2009-05-29 206.261261 | 208.823822 | 205.555557 | 208.823822 | 208.823822 | 5291100 | . train_data[&quot;Year&quot;]= train_data.index.year train_data[&quot;Month&quot;]= train_data.index.month train_data[&quot;Weekday name&quot;]= train_data.index.day_name() . train_data.head() . Open High Low Close Adj Close Volume Year Month Weekday name . Date . 2009-05-22 198.528534 | 199.524521 | 196.196198 | 196.946945 | 196.946945 | 3433700 | 2009 | 5 | Friday | . 2009-05-26 196.171173 | 202.702698 | 195.195190 | 202.382385 | 202.382385 | 6202700 | 2009 | 5 | Tuesday | . 2009-05-27 203.023026 | 206.136139 | 202.607605 | 202.982986 | 202.982986 | 6062500 | 2009 | 5 | Wednesday | . 2009-05-28 204.544540 | 206.016022 | 202.507507 | 205.405411 | 205.405411 | 5332200 | 2009 | 5 | Thursday | . 2009-05-29 206.261261 | 208.823822 | 205.555557 | 208.823822 | 208.823822 | 5291100 | 2009 | 5 | Friday | . Exploratory Data Analysis . plt.figure(figsize=(16,8)) train_data[&quot;Adj Close&quot;].plot() train_data[&quot;Adj Close&quot;].rolling(window=90).mean().plot() plt.xlabel(&quot;Date&quot;) plt.ylabel(&quot;Close Price (USD)&quot;) plt.title(&quot;Close Price History&quot;) . Text(0.5, 1.0, &#39;Close Price History&#39;) . train_data.boxplot(column= [&quot;Adj Close&quot;]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f6843329a90&gt; . train_data[[&quot;Open&quot;,&quot;High&quot;,&quot;Low&quot;,&quot;Close&quot;,&quot;Volume&quot;]].plot(figsize=(12,8),subplots=True) . array([&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f68432bf910&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f6842df0990&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f6842da9d50&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f6842d6b150&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f6842d24510&gt;], dtype=object) . corr=train_data.corr() sns.heatmap(corr, annot= True) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f6842b5d650&gt; . it shows that Adj. Close, Open, High, Close and Low have a very high correlation. So we have to drop these values. | . Preprocessing . Adding new Features to the Dataset . Due to the high correlation, it is more convinient to add some features to the dataset. | HL_PCT calculates for the high-low percentage for each day and the PCT_change calculatesfor the open-close percentage for each day. | . train_data[&quot;HL_PCT&quot;]= (train_data[&quot;High&quot;] - train_data[&quot;Low&quot;])/ train_data[&quot;Low&quot;]*100 #High Low percentage train_data[&quot;PCT_change&quot;]= (train_data[&quot;Close&quot;]- train_data[&quot;Open&quot;])/train_data[&quot;Open&quot;]*100 #Open Close percentage . train_data.isnull().sum() . Open 0 High 0 Low 0 Close 0 Adj Close 0 Volume 0 Year 0 Month 0 Weekday name 0 HL_PCT 0 PCT_change 0 dtype: int64 . df = train_data[[&quot;Adj Close&quot;, &quot;HL_PCT&quot;, &quot;PCT_change&quot;, &quot;Volume&quot;]] . forecast_out = int(math.ceil(len(df)*0.05)) #forcasting out 5% of the entire dataset print(forecast_out) . 117 . df[&quot;Prediction&quot;]= df[&quot;Adj Close&quot;].shift(-forecast_out) df[&quot;Prediction&quot;] . /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy &#34;&#34;&#34;Entry point for launching an IPython kernel. . Date 2009-05-22 275.825836 2009-05-26 281.536530 2009-05-27 283.663666 2009-05-28 285.565552 2009-05-29 284.209198 ... 2018-08-23 NaN 2018-08-24 NaN 2018-08-27 NaN 2018-08-28 NaN 2018-08-29 NaN Name: Prediction, Length: 2335, dtype: float64 . Model Building . from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LinearRegression, Ridge from sklearn.ensemble import RandomForestRegressor from sklearn.svm import SVR from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_log_error . #Now we scale the data between -1 and 1 in order to pu all the column in the same range. #We will be using the StandardSacler fuction from the preprocessing module of sklearn librairy scaler = StandardScaler() . X = np.array(df.drop([&quot;Prediction&quot;],1)) scaler.fit(X) X = scaler.transform(X) . Picking The Data to be Predicted . We have succesfully scaled the data. | Previously we&#39;ve included a new column called &quot;Prediction&quot; to our dataset wich contains forcasted out values. -Also, we&#39;ve made a room for 117 new predictions. -So we are going to pick all rows in the dataset excluding the remaining 117 rows as our training data, and use the remaining 117 rows as the data to be predicted. | . X_Predictions = X[-forecast_out:]#data to be predicted X = X[:-forecast_out]#data to be trained . X.shape . (2218, 4) . df.dropna(inplace=True) . /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy . y = np.array(df[&quot;Prediction&quot;]) y.shape . (2218,) . X_train, X_test, y_train, y_test = train_test_split(X, y , test_size = 0.2, random_state=42) . Training the Models . Applying different linear regression models and see which gives the best accuracy. | . Linear Regression . lr = LinearRegression() lr.fit(X_train,y_train) . LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False) . lr_confidence = lr.score(X_test, y_test) lr_confidence . 0.9558130106873121 . Random Forest . rf = RandomForestRegressor() rf.fit(X_train,y_train) . RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion=&#39;mse&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False) . rf_confidence = rf.score(X_test, y_test) rf_confidence . 0.9736712815829767 . Ridge . rg = Ridge() rg.fit(X_train, y_train) . Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None, normalize=False, random_state=None, solver=&#39;auto&#39;, tol=0.001) . rg_confidence = rg.score(X_test, y_test) rg_confidence . 0.9558232815861608 . Support Vector Regressor (SVR) . svr = SVR() svr.fit(X_train, y_train) . SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=&#39;scale&#39;, kernel=&#39;rbf&#39;, max_iter=-1, shrinking=True, tol=0.001, verbose=False) . svr_confidence = svr.score(X_test, y_test) svr_confidence . 0.6725552349692899 . Visualizing wich model have the best accuracy . names =[&quot;Linear Regression&quot;, &quot;Random Forest&quot;, &quot;Ridge&quot;, &quot;SVR&quot;] columns = [&quot;models&quot;, &quot;accuracy&quot;] scores = [lr_confidence, rf_confidence, rg_confidence, svr_confidence] alg_vs_score = pd.DataFrame([[x,y] for x,y in zip(names,scores)], columns= columns) alg_vs_score . models accuracy . 0 Linear Regression | 0.955813 | . 1 Random Forest | 0.973671 | . 2 Ridge | 0.955823 | . 3 SVR | 0.672555 | . sns.barplot(data = alg_vs_score, x= &quot;models&quot;, y=&quot;accuracy&quot;) plt.title(&#39;Performance of Different Models&#39;) plt.xticks(rotation=&#39;vertical&#39;) . (array([0, 1, 2, 3]), &lt;a list of 4 Text major ticklabel objects&gt;) . Adding the Predicted Data To The Dataset . last_date = df.index[-1] last_unix = last_date.timestamp() one_day = 86400 next_unix = last_unix + one_day . forecast_set = rf.predict(X_Predictions) df[&quot;Forecast&quot;] = np.nan . /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy . for i in forecast_set: next_date = datetime.datetime.fromtimestamp(next_unix) next_unix += 86400 df.loc[next_date] = [np.nan for _ in range(len(df.columns)-1)]+[i] . /usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy iloc._setitem_with_indexer(indexer, value) . plt.figure(figsize=(18, 8)) df[&#39;Adj Close&#39;].plot() df[&#39;Forecast&#39;].plot() plt.legend(loc=4) plt.xlabel(&#39;Date&#39;) plt.ylabel(&#39;Price&#39;) plt.show() . df[&#39;Forecast&#39;].plot() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f6832015d10&gt; .",
            "url": "https://datasiastic.github.io/Projects_Portfolio/time-serie/linear-regression/svr/ridge/random-forest/googl/scikit-learn/2021/09/28/TS_FC_GOOGL.html",
            "relUrl": "/time-serie/linear-regression/svr/ridge/random-forest/googl/scikit-learn/2021/09/28/TS_FC_GOOGL.html",
            "date": " • Sep 28, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Building a Stock Price Prediction Model with KNN",
            "content": "Importing dependecies and Apple dataset . import pandas as pd from datetime import datetime import pandas_datareader.data as web from pandas import Series, DataFrame start_date = &quot;2010-01-01&quot; today = datetime.today().strftime(&quot;%Y-%m-%d&quot;) df = web.DataReader(&quot;AAPL&quot;, &#39;yahoo&#39;, start = start_date, end = today) df.tail() . High Low Open Close Volume Adj Close . Date . 2020-12-23 132.429993 | 130.779999 | 132.160004 | 130.960007 | 88223700.0 | 130.960007 | . 2020-12-24 133.460007 | 131.100006 | 131.320007 | 131.970001 | 54930100.0 | 131.970001 | . 2020-12-28 137.339996 | 133.509995 | 133.990005 | 136.690002 | 124486200.0 | 136.690002 | . 2020-12-29 138.789993 | 134.339996 | 138.050003 | 134.869995 | 121047300.0 | 134.869995 | . 2020-12-30 135.990005 | 133.399994 | 135.580002 | 133.720001 | 96292700.0 | 133.720001 | . Calculating the Moving Average . df[&quot;M.Avg&quot;]= df[&quot;Adj Close&quot;].rolling(window=100).mean() df.tail() . High Low Open Close Volume Adj Close M.Avg . Date . 2020-12-23 132.429993 | 130.779999 | 132.160004 | 130.960007 | 88223700.0 | 130.960007 | 117.860380 | . 2020-12-24 133.460007 | 131.100006 | 131.320007 | 131.970001 | 54930100.0 | 131.970001 | 118.087289 | . 2020-12-28 137.339996 | 133.509995 | 133.990005 | 136.690002 | 124486200.0 | 136.690002 | 118.357437 | . 2020-12-29 138.789993 | 134.339996 | 138.050003 | 134.869995 | 121047300.0 | 134.869995 | 118.571120 | . 2020-12-30 135.990005 | 133.399994 | 135.580002 | 133.720001 | 96292700.0 | 133.720001 | 118.799109 | . Visualizing the data price &amp; M.avg . import matplotlib.pyplot as plt plt.style.use(&quot;seaborn&quot;) import matplotlib as mpl mpl.rc(&#39;figure&#39;, figsize=(16, 8)) df[[&quot;Adj Close&quot;,&quot;M.Avg&quot;]].plot() plt.show() . Calculating &amp; Visualizing the Daily Returns . returns = df[&quot;Adj Close&quot;].pct_change() returns= pd.DataFrame(returns) returns . Adj Close . Date . 2010-01-04 NaN | . 2010-01-05 0.001729 | . 2010-01-06 -0.015906 | . 2010-01-07 -0.001849 | . 2010-01-08 0.006648 | . ... ... | . 2020-12-23 -0.006976 | . 2020-12-24 0.007712 | . 2020-12-28 0.035766 | . 2020-12-29 -0.013315 | . 2020-12-30 -0.008527 | . 2768 rows × 1 columns . returns.plot() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f0394b8c978&gt; . Bulding the portfolio . dfcomp = web.DataReader([&#39;AAPL&#39;,&#39;AMZN&#39;,&#39;NFLX&#39;,&#39;GOOG&#39;,&#39;MSFT&#39;,&#39;FB&#39;], &#39;yahoo&#39;, start = start_date, end = today)[&quot;Adj Close&quot;] dfcomp . Symbols AAPL AMZN NFLX GOOG MSFT FB . Date . 2010-01-04 6.593426 | 133.899994 | 7.640000 | 312.204773 | 24.105360 | NaN | . 2010-01-05 6.604825 | 134.690002 | 7.358572 | 310.829926 | 24.113148 | NaN | . 2010-01-06 6.499768 | 132.250000 | 7.617143 | 302.994293 | 23.965164 | NaN | . 2010-01-07 6.487752 | 130.000000 | 7.485714 | 295.940735 | 23.715933 | NaN | . 2010-01-08 6.530883 | 133.520004 | 7.614286 | 299.885956 | 23.879499 | NaN | . ... ... | ... | ... | ... | ... | ... | . 2020-12-23 130.960007 | 3185.270020 | 514.479980 | 1732.380005 | 221.020004 | 268.109985 | . 2020-12-24 131.970001 | 3172.689941 | 513.969971 | 1738.849976 | 222.750000 | 267.399994 | . 2020-12-28 136.690002 | 3283.959961 | 519.119995 | 1776.089966 | 224.960007 | 277.000000 | . 2020-12-29 134.869995 | 3322.000000 | 530.869995 | 1758.719971 | 224.149994 | 276.779999 | . 2020-12-30 133.720001 | 3285.850098 | 524.590027 | 1739.520020 | 221.679993 | 271.869995 | . 2768 rows × 6 columns . title = &#39;portfolio Adj. close price history&#39; #Create and plot the graph for c in dfcomp.columns.values: plt.plot(dfcomp[c], label = c) #plt.plot(dfcomp[[&quot;AAPL&quot;,&quot;AMZN&quot;,&quot;NFLX&quot;,&quot;FB&quot;,&quot;MSFT&quot;]]) plt.title(title) plt.xlabel(&quot;Date&quot;, fontsize= 16) plt.ylabel(&quot;Adj.price USD ($)&quot;, fontsize= 16) plt.legend(dfcomp.columns.values, loc=&#39;upper left&#39;) plt.show() . Calculating the portfolio correlation . pfrets = dfcomp.pct_change() pfcorr = pfrets.corr() pfcorr . Symbols AAPL AMZN NFLX GOOG MSFT FB . Symbols . AAPL 1.000000 | 0.448198 | 0.251906 | 0.527316 | 0.556531 | 0.393691 | . AMZN 0.448198 | 1.000000 | 0.412491 | 0.575691 | 0.529254 | 0.451393 | . NFLX 0.251906 | 0.412491 | 1.000000 | 0.330493 | 0.303393 | 0.286692 | . GOOG 0.527316 | 0.575691 | 0.330493 | 1.000000 | 0.611136 | 0.495600 | . MSFT 0.556531 | 0.529254 | 0.303393 | 0.611136 | 1.000000 | 0.415975 | . FB 0.393691 | 0.451393 | 0.286692 | 0.495600 | 0.415975 | 1.000000 | . plt.scatter(pfrets.GOOG, pfrets.AMZN) plt.xlabel(&#39;Returns GOOG&#39;) plt.ylabel(&#39;Returns NFLX&#39;) . Text(0, 0.5, &#39;Returns NFLX&#39;) . from pandas.plotting import scatter_matrix scatter_matrix(pfrets, diagonal=&#39;kde&#39;, figsize=(10, 10)) . array([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f03948f6630&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f0394732780&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f0394b5add8&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f0394a5f470&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f0394a9aac8&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f0394aa2160&gt;], [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f0394ae57b8&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f039478bdd8&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f039478be48&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f03949aab00&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f03947e8198&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f03948197f0&gt;], [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f039470be48&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f03946c74e0&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f0394678b38&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f03946341d0&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f03945e5828&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f039461d278&gt;], [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f03945cc9e8&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f0394589198&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f0394538908&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f03944f60b8&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f03944a2828&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f03944d4f98&gt;], [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f039448d748&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f039443deb8&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f03943fa668&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f03943a7dd8&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f0394362588&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f0394390cf8&gt;], [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f039434b4a8&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f03942f9c18&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f0394b4a550&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f03949b9278&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f039463ccf8&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f0394946160&gt;]], dtype=object) . plt.imshow(pfcorr, cmap=&#39;hot&#39;, interpolation=&#39;none&#39;) plt.colorbar() plt.xticks(range(len(pfcorr)), pfcorr.columns) plt.yticks(range(len(pfcorr)), pfcorr.columns); . plt.scatter(pfrets.mean(), pfrets.std()) plt.xlabel(&#39;Expected returns&#39;) plt.ylabel(&#39;Risk&#39;) for label, x, y in zip(pfrets.columns, pfrets.mean(), pfrets.std()): plt.annotate( label, xy = (x, y), xytext = (20, -20), textcoords = &#39;offset points&#39;, ha = &#39;right&#39;, va = &#39;bottom&#39;, bbox = dict(boxstyle = &#39;round,pad=0.5&#39;, fc = &#39;yellow&#39;, alpha = 0.5), arrowprops = dict(arrowstyle = &#39;-&gt;&#39;, connectionstyle = &#39;arc3,rad=0&#39;)) . Predicting Stocks Price . Feature Engineering . dfreg = df[[&#39;Adj Close&#39;,&#39;Volume&#39;]] dfreg[&#39;HL_PCT&#39;] = (df[&#39;High&#39;] - df[&#39;Low&#39;]) / df[&#39;Close&#39;] * 100.0 dfreg[&#39;PCT_change&#39;] = (df[&#39;Close&#39;] - df[&#39;Open&#39;]) / df[&#39;Open&#39;] * 100.0 dfreg . /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy This is separate from the ipykernel package so we can avoid doing imports until . Adj Close Volume HL_PCT PCT_change . Date . 2010-01-04 6.593426 | 493729600.0 | 0.990606 | 0.271752 | . 2010-01-05 6.604825 | 601904800.0 | 1.091520 | -0.102519 | . 2010-01-06 6.499768 | 552160000.0 | 2.123523 | -1.590633 | . 2010-01-07 6.487752 | 477131200.0 | 1.400893 | -0.552538 | . 2010-01-08 6.530883 | 447610800.0 | 1.386924 | 0.798864 | . ... ... | ... | ... | ... | . 2020-12-23 130.960007 | 88223700.0 | 1.259922 | -0.907988 | . 2020-12-24 131.970001 | 54930100.0 | 1.788286 | 0.494969 | . 2020-12-28 136.690002 | 124486200.0 | 2.801962 | 2.015073 | . 2020-12-29 134.869995 | 121047300.0 | 3.299471 | -2.303519 | . 2020-12-30 133.720001 | 96292700.0 | 1.936892 | -1.371884 | . 2768 rows × 4 columns . Pre-processing &amp; Cross Validation . We will clean up and process the data using the following steps before putting them into the prediction models: . Drop missing value | Separating the label here, we want to predict the AdjClose | Scale the X so that everyone can have the same distribution for linear regression | Finally We want to find Data Series of late X and early X (train) for model generation and evaluation Separate label and identify it as y | Separation of training and testing of model by cross validation train test split | . import numpy as np import math from sklearn import preprocessing from sklearn.model_selection import train_test_split from sklearn import svm #Drop missing value dfreg.fillna(value= -99999, inplace=True) #Create a variable for predicting &#39;n&#39; days out into the future forecast_out = int(math.ceil(len(dfreg)* 0.01)) #Create another column (the target or the dependent variable) shifted &#39;n&#39; units up dfreg[&#39;Prediction&#39;]= df[[&#39;Adj Close&#39;]].shift(-forecast_out) . /usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:4327: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy downcast=downcast, /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy . X = np.array(dfreg.drop([&#39;Prediction&#39;],1)) . X = preprocessing.scale(X) . /usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_data.py:173: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features. warnings.warn(&#34;Numerical issues were encountered &#34; . X = X [:-forecast_out] X . array([[-1.02057093, 0.90509935, -0.78641583, 0.18451687], [-1.02011453, 1.37688182, -0.70432877, -0.0910894 ], [-1.02432078, 1.15993076, 0.13513761, -1.18691057], ..., [ 3.53198167, -0.85051957, 0.32811589, 0.83893735], [ 3.4955472 , -0.92427756, -0.42714281, -0.11415263], [ 3.44109587, -0.91533213, -0.33790859, -0.37568687]]) . x_forecast = X[-forecast_out:] x_forecast . array([[ 3.68755825, -0.20049764, 2.26571997, 2.64632548], [ 3.55566121, -0.10409719, 2.26337423, -2.46687302], [ 3.55925835, -0.5893691 , 0.69660037, 0.10003536], [ 3.5400734 , -0.75729195, 0.46310651, 1.21873625], [ 3.47252593, -0.74492946, 0.28043004, -1.38781467], [ 3.35102107, -0.72205236, 1.74624838, -2.45874384], [ 3.41217335, -0.70554753, 0.72675126, 0.81458001], [ 3.38659352, -0.85591447, -0.01921017, 0.11063985], [ 3.34182814, -0.80339592, 0.83228552, -1.08145273], [ 3.31345057, -0.88807194, 0.01288473, -0.86972085], [ 3.31385012, -0.76038192, 1.00258356, 0.65613284], [ 3.3758015 , -0.8457492 , 0.31929076, 0.6921576 ], [ 3.15997008, -0.62044105, 1.57521214, -2.47980977], [ 3.32464154, -0.61088374, 1.74420026, 1.91759423], [ 3.06644355, -0.41836196, 1.59845966, -1.47430301], [ 3.0628461 , -0.71233717, 0.9205612 , -0.24506488], [ 3.12959417, -0.778814 , 0.44063649, 0.50818388], [ 3.30985313, -0.64531038, 0.70054945, 0.50697943], [ 3.47292578, -0.69698459, 0.28710278, 0.65866771], [ 3.46752091, -0.74901118, 0.51179591, 0.21468041], [ 3.37263139, -0.5743096 , 2.56167302, -2.57001926], [ 3.35861825, -0.64623541, 0.83469646, 0.252062 ], [ 3.49955091, -0.7584442 , 0.5793967 , 1.42964339], [ 3.48834039, -0.79827447, -0.25479513, -0.26799534], [ 3.4903424 , -0.89239266, -0.36448924, -0.12657191], [ 3.53198167, -0.85051957, 0.32811589, 0.83893735], [ 3.4955472 , -0.92427756, -0.42714281, -0.11415263], [ 3.44109587, -0.91533213, -0.33790859, -0.37568687]]) . y = np.array(dfreg[&#39;Prediction&#39;])[:-forecast_out] y . array([ 6.17349958, 6.26654387, 6.24035645, ..., 136.69000244, 134.86999512, 133.72000122]) . X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2) . K Nearest Neighbor (KNN) . This KNN uses feature similarity to predict values of data points. This ensures that the new point assigned is similar to the points in the data set. To find out similarity, we will extract the points to release the minimum distance (e.g: Euclidean Distance). . from sklearn.neighbors import KNeighborsRegressor . clfknn = KNeighborsRegressor(n_neighbors=2) clfknn.fit(X_train, y_train) . KNeighborsRegressor(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;minkowski&#39;, metric_params=None, n_jobs=None, n_neighbors=2, p=2, weights=&#39;uniform&#39;) . Evaluation . A simple quick and dirty way to evaluate is to use the score method in each trained model. The score method finds the mean accuracy of self.predict(X) with y of the test data set. . confidenceknn = clfknn.score(X_test, y_test) print(&quot;The KNN regression confidence is: &quot;, confidenceknn) . The KNN regression confidence is: 0.9531964601858879 . Predicting the Stock Prices . forecast_set = clfknn.predict(x_forecast) #Create an empty column for the forecast result print(forecast_set) . [117.61442947 114.1742363 121.0399971 123.52500153 115.4559021 119.61592484 125.06499863 120.77249908 118.98090363 115.73831177 125.06499863 119.65032196 119.61592484 118.26601028 117.05093765 120.95000076 119.65032196 121.71532059 119.65032196 114.7003212 112.96133041 121.89031982 123.52500153 132.34000397 131.46500397 119.45999908 131.46500397 132.34000397] . Plotting the Prediction . predictions = forecast_set valid = df[X.shape[0]:] valid[&quot;Prediction&quot;] = predictions plt.figure(figsize=(16,8)) plt.title(&quot;Apple&quot;) plt.xlabel(&quot;Days&quot;) plt.ylabel(&quot;Adj. Close USD ($)&quot;) plt.plot(df[&quot;Adj Close&quot;]) plt.plot(valid[[&quot;Adj Close&quot;,&quot;Prediction&quot;]]) plt.legend([&quot;Orig&quot;,&quot;Valid&quot;,&quot;Pred&quot;]) plt.show() . /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy &#34;&#34;&#34; .",
            "url": "https://datasiastic.github.io/Projects_Portfolio/knn/aapl/scikit-learn/2021/09/13/SPP_with_KNN.html",
            "relUrl": "/knn/aapl/scikit-learn/2021/09/13/SPP_with_KNN.html",
            "date": " • Sep 13, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". I&#39;m a Finance Graduate Interested In Machine Learning &amp; its applications in Finance.On this blog page I publish some applications &amp; projects I&#39;ve worked on,and useful ressources to get started in Data Science &amp; Machine Learning .",
          "url": "https://datasiastic.github.io/Projects_Portfolio/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://datasiastic.github.io/Projects_Portfolio/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}